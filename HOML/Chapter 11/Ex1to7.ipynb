{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "    - No, since it will not break the symmetry and all will be updated similarly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Is it OK to initialize the bias terms to 0?\n",
    "    - Yes, even if you initialize them like the weights would not make much difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Name three advantages of the SELU activation function over ReLU\n",
    "    - Can take on negative values -> avg output of neuron closer to 0 than RELU -> alleviate vanishing gradients\n",
    "    - Has non-zero derivative -> avoid dying units\n",
    "    - \"When the conditions are right:\n",
    "        - If the model is sequential\n",
    "        - The weights are initialized using LeCun initialization\n",
    "        - The inputs are standardized\n",
    "        - There’s no incompatible layer or regularization, such as dropout or ℓ1 regularization)\n",
    "    \n",
    "    -> ensures the model is self-normalized, which solves the exploding/vanishing gradients problems\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "    - SELU: we can satistfy its condition\n",
    "    - Leaky ReLU: care about runtime latency\n",
    "    - Other variants of Leaky ReLU: have time for cross-validation to evaluate\n",
    "    - ReLU: Speed is priority, and ReLU is simple to use\n",
    "    - tanh: (mostly for output layers) Need output to be in range \\[-1,1\\] (not used much these days)\n",
    "    - Sigmoid: (mostly for output layers) Need to estimate probability (e.g: binary classification), rarely used in hidden layers\n",
    "    - sofmax: (mostly for output layers) Need to estimate probabilities for mutually exclusive classes, rarely used in hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n",
    "    - The momentum will grow to large causing the optimizer to shot out of the global optimum, moving back and forth and takes a lot of time to converge than a smaller one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Name three ways you can produce a sparse model.\n",
    "    - Train model normally, zero out tiny weights\n",
    "    - Apply $ℓ_1$ regularization during training (pushes the optimizer toward sparsity)\n",
    "    - Use TensorFlow Model Optimization Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?\n",
    "    - Droupout does slow down training (generally roughly by a factor of two) but not for inference (because active only in training).\n",
    "    - MC Droupout does slow down for inference since it has to sample multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
