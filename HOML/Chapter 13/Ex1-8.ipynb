{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why would you want to use the Data API?\n",
    "    - Datasets are usually very large and we can't load all of it into RAM, causing inefficient ingesting and preprocessing\n",
    "    - Data API makes things easy since it provides many operations handling data (reading, transforming,...) supporting multithreading, queuing, batching,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What are the benefits of splitting a large dataset into multiple files?\n",
    "    - Since dataset is very large and not fit RAM so some operations might not work effectively (e.g: shuffling)\n",
    "    - So we try to split dataset to multiple files as a divide and conquer solution\n",
    "    - If the data is split across multiple files spread across multiple servers, it is possible to download several files from different servers simultaneously, which improves the bandwidth usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. During training, how can you tell that your input pipeline is the bottleneck? What can you do to fix it?\n",
    "    - How:\n",
    "        - Using TensorBoard to visualize profiling data:\n",
    "            - GPU not fully utilized -> **maybe input pipeline is bottleneck**\n",
    "    - Fix:\n",
    "        - Using `prefetch()` to load batch before it actually needs it\n",
    "        - Optimize preprocessing code\n",
    "        - Saving dataset to multiple TFRecord files\n",
    "        - Preprocess ahead of time\n",
    "        - Update machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Can you save any binary data to a TFRecord file, or only serialized protocol buffers?\n",
    "    - Yes, we can save any.\n",
    "    - But we should save serialized since it provides benifits as:\n",
    "        - Read easily multiplatform\n",
    "        - Updatable definitions\n",
    "        - Backward-compatible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Why would you go through the hassle of converting all your data to the Example protobuf format? Why not use your own protobuf definition?\n",
    "    - TF has already have some operations to parse it\n",
    "    - It is flexible enough to represent most of the data you might encounter\n",
    "    - If self-define, we have to compile it and use `tf.io.decode_proto()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. When using TFRecords, when would you want to activate compression? Why not do it systematically?\n",
    "    - Active if:\n",
    "        - TFRecord files need to be downloaded by the training script (compression reduce size -> smaller download time)\n",
    "    - Not if:\n",
    "        - Data is in local drive\n",
    "        - Not wasting time for CPU to decompress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Data can be preprocessed directly when writing the data files, or within the tf.data pipeline, or in preprocessing layers within your model, or using TF Transform. Can you list a few pros and cons of each option?\n",
    "    - When writing the data files:\n",
    "        - Pros: \n",
    "            - Training script will run faster (don't have to preprocess on the flu)\n",
    "            - Smaller size\n",
    "            - Helpful for inspecting or archiving\n",
    "        - Cons:\n",
    "            - Not easy to experiment with various preprocessing logics\n",
    "            - Take many disk spaces if want to data augmentation\n",
    "            - Have to add preprocessing code to preprocess the data before it's passed to the model\n",
    "    - Processed with `tf.data`:\n",
    "        - Pros:\n",
    "            - Easier to tweak preprocessing logic, data augmentation\n",
    "            - Easily build highly efficient preprocessing pipelines (multithreading, prefetching,..)\n",
    "        - Cons:\n",
    "            - Slow down training\n",
    "            - Training instance will be processed once/ epoch\n",
    "            - Model still expect for preprocessed data\n",
    "    - Process within the model:\n",
    "        - Pros:\n",
    "            - Only have to write preprocessing code for training/ inference once and deploy to anywhere\n",
    "            - Hard to run wrong preprocessing logic (in is embedded to the model already)\n",
    "        - Cons:\n",
    "            - Slow down training\n",
    "            - Training instance will be processed once/ epoch\n",
    "    - TF Transform:\n",
    "        - Pros:\n",
    "            - Preprocessed data is materialized\n",
    "            - Each instance is preprocess once\n",
    "            - Generated automatically -> write preprocessing code once\n",
    "        - Cons:\n",
    "            - Learn API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Name a few common techniques you can use to encode categorical features.What about text?\n",
    "    - Ordinal Encoding\n",
    "    - One Hot Encoding\n",
    "    - Embedding\n",
    "    - Text:\n",
    "        - Bag of words\n",
    "        - TF-IDF\n",
    "        - Word embeddings\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
